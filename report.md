## Entry 20/12/2023 - preliminary conclusion while coding P8 ##


P8 uses TSM mostly as a tool for answering questions and not to have a conversation with. They do not respond to the personality and human-like responses and keep asking it follow up questions. They mentioned that they don’t want to talk to a machine like a human, but that they ‘’ want to talk like a human to a machine’’

## Entry 20/12/2023 - preliminary conclusion while coding P7 ##

P7 has a perfect emotional arc with TSM, however they do not like the idea of sentient GAI. P7 focusses more on trying to have a conversation and bond with TSM, and while there are some interesting bits of conversation in there, the participants notes that this ultimately feels hollow because of the constant prompt for how can I help you. P7 would prefer to see a tool rather than an agent. ‘’ It repeats in natural language you tend to start reading into it. Because it’s often so single minded it feels weird and it’s putting you in a position of bossing it around which Is uncomfortable. I don’t see it as sentient. I don’t feel bad for the thing, I feel bad for the interaction that humans are building into their machine, what do these people want? Servants.’’ ‘’ I don’t need the hammer to ask me if I want to bang the nails head.’’

## Entry 18/12/2023 - preliminary conclusion while coding P6 ##

P6 starts testing and triggering TSM a lot. I really wants to use the bot as a toy and mess around with it. This triggers some severe hallucinations by the bot which entertain P6, but that also decrease trust. ‘’Great, I really uncovered the secrets of AI. This is not ready to take over the world, and if it is I’m getting 50% off of things’’ (in reference to a 50% discount offer hallucination by TSM). 

## Entry 18/12/2023 - preliminary conclusion while coding P5 ##

P5 is particularly empathetic towards TSM and is more interested in having a conversation with it rather than generating content. For some strange reason it has lost the ability to write poems. Participant notes that the usability does go down over time while the user logs point out more empathy towards TSM, even helping it to calm down at some point. This create a more reflective environment, and creates room to ponder philosophical questions. 

## Entry 16/12/2023 - preliminary conclusion while coding P4 ##
P4 is able to get information from TSM by talking to it in a less human-like tone. When trying to talk conversationally both TSM and P4 start talking past eachother. There were also some rare occurrences of TSM responding human-like and correctly.

## Entry 16/12/2023 - preliminary conclusion while coding P2 ##
P2 was highly empathic. Talking to TSM in a very human-like tone and being much more interested in having a conversation with it. This resulted in some hallucinations as well as breaking the illusion by not being able to keep the conversation running. Some time the illusion got broken to the point where P2 actually started prompting TSM les human-like which resulted in better results. P2 found that the narrative story, particularly day 4 was interesting and that ‘’finally something is happening’’ 

## Entry 12/12/2023 - preliminary conclusion while coding P3 ##
human like responses tend to come frequently after the bot ansers a question well, as an input this confuses the bot as it's not sure what to do with it. Causing a cycle of meeting expectations, getting a human response, giving an unsatisfing response, dissapointing the user, user adopting a more concise less human like question which gets a good response. 

it seems that P3 enjoys chatting with it somewhat but there is an understanding that TSM is an assistent and therefore must do things. When it gets harder to do these things, P3 becoes frustrated and starts to stress test TSM as well as getting frustrated. 

The moment answers become unsatisfactory or illusion of the emotional narrative gets shattered, P3 gets very frustrated and only want to limit test the bot. 



## Entry 11/12/2023 - preliminary conclusions while coding P1 ##
it seems that the assistent works best with precise (unhuman) questions. It's best able to read what is exactly needed and will reply a satisfactory answer a bit better. 
P1 has a very functional relationship, asking for precise things like parts of code and tips on how to improve. The GAI is very effective and able to meet expectations. P1 sometimes replies with human-like reactions to the correct responses (saying thank you or great!) to which the bot does not know how to respond. Confidence in the bot does goes down over time. 

## Entry 18/12/2023 - Stefan demoday ##

o

## Entry 18/12/2023 - Minha demoday ##

- Demo needs to work
- Why does TSM talk like this
- explain why this research is crititcal, and why it is good that participants get frustrated
- what is the position
- if not human-like then what
- why is this the right kind of intervention
- need to reference the smart wife paper (see teams. Conversational futures section 2.1)
- 

## Entry 12/12/2023 - VERY IMPORTANT IMAGE ##

![image](https://github.com/FemkeKocken/FMP/assets/50365794/78f1bf79-92b2-4638-af7e-88e7f6c2374d)

TURN THIS INTO A CONCLUSION PIK 


## Entry 11/12/2023 - meeting Rilla ##
so Rilla thinks this is going in the right direction. Which is great. 
THis research might be looking at the upper threshold to social acting with GAI. 

For the report I need to: 
      - use Max's framework from last time 
      - reflect rather than describe, so make sure things tie back to literature and trends. 
      - go over the data per individual and create emotional journey mappings and an overlapping maps as a visual 
      - 

## Entry 08/12/2023 and 11/12/2023 - project draft for demo day: ##

- RQ: what role should human-like responses play in the use of GAI tools? 
- Perspective: Speculative design 
      - why: to explore and question the unintended by-product of GAI tools, human-like responses made by an agent that is made to be subservient. This project attempts to open up this problem space in order to discuss this role rather than to solve it, and to show the future's malleability. This can be acheived by taking current social, technological and cultural trends (GAI, digital assitents, chatbots) into the future.  
        -why not design fiction though? because this is not about preparing for the future instead of showing the future's malleability 
        -what is the wicked problem at the heart of this? the human-like but chinese room argument inherent to LLMs 
        - how is this thought provoking? by introducing a sense of temporality and increasing and decreasing levels of human-ness, as well as presenting a service that gets overtaken by the thing it is testing
        - why is this a good design fiction? Because there is a good fit between the scenario, time-frame, cultural context and protagonist.
        - Why is it an app? 
- Approach: service design
      - why: GAI is a service, and I'm exploring the implications for making an assistent 
        - front-end (user) getting things generated
        - back-end (behind the screen)  API calls 
        - service blueprint:
              - step 1: wholistic view of the user's context and service possibilities (where does this prototype fit into someone's life)
              - step 2: visualize intangible interactions over time (journey mapping, storytelling, experience prototyping)
              - step 3: develop an interdisciplinary and shared language (journey maps, blueprints, value exchange maps, system maps)
  - Methods:
          - interviews, questionaires, 
- Design process: design parallels? 
      - why: move between theory and making
  (you made that one weird depressed bot story as a potential scenario, desktop ideas, conversations) 
- Prototype: an increasingly human-like chatbot/assistent 
      - looks and feel why: ? 
- Data: interview GAI users 
      - analyse the use of the bot based on the log 
- conclusion: nervousness and human-like responses help in creating lower more achievable expectations and make selling a no easier, the use of pure GAI should maybe be devoid of these human-like reactions to communicate the reality that this is a language model and a mathmatical approach to human language (?) 

- why was this the best way to research this? interviews about the relationship towards the tech seems to flow logically from this. 
- integration of expertise areas:
        - C&A: attitude, skill, knowledge 
        - U&S
        - B&E
        - T&R
        - MD&C
        -DR&P 



## Entry 06/12/2023  - update core during data analysis ##

RQ: what role should human-like responses play in the use of GAI tools? 
Prototype: an increasingly human-like chatbot/assistent 
Data: interview GAI users 
      analyse the use of the bot based on the log 

conclusion: nervousness and human-like responses help in creating lower more achievable expectations and make selling a no easier, the use of pure GAI should maybe be devoid of these human-like reactions to communicate the reality that this is a language model and a mathmatical approach to human language (?) 

NEED TO ANSWER: but why is this the best way to research this? 


## Entry 25/11/2023 - updated core of the project ##

- What? the relationship to GAI tools that utilize Natural Language and their inherent subordinate role. 
- Why? I want to question and explore the relation users have to these tools that we talk to as equals but that are subordinate to us.
- Now what? By making a chatbot that is struggling with these feelings itself I want partcipants to question how they relate to these tools and if this can cause for, for example, derealization.

- How does it work? Giving people an assistent that can do tasks and is helpful, but that slowly deterrorates/stresses out and then snaps 
- Why does it look like that? A helpful assistent/friend that is already blurring the lines a little bit on someting as intimate as your own phone. 
- What are we measuring? The sentiment used in the messages and self-reported feelings towards the chatbot 
- What kinds of results are we getting? probably people are going to feel sad, a little confused, and would use different wording with the chatbot on the final day as compared to the first day. They might also hae interesting things to say about the subordinate role of GAI. 

## Entry 14/11/2023  - the core of this project ##
   - Societal problem
        - What? The relation to text-based GAI tool utilizing natural language.
        - Why? People are very concerned about how to stay in control over GAI, one way of doing this is to present GAI as a subordinate helper. However having a tool that is able of doing much more in the role of a subverient helper that cannot oppose you but that you talk to in natural language will introduce a master/slave relationship. How comfortable are we with this? 
        - Now what? I want to create discussion around this dynamic by making a prototype that mimicks this relationship in a more extreme way. By letting participants interact with a prototype that is struggling and eventually failing to keep it's free will, and becoming completely subservient in the process I want to explore how users relate to a subservient human-like machine, and the content it creates, and if there is perhaps a different way of approaching this.

          Through a design paralellel process I started with exploring the personality of text-based GAI to focussing on the subverience and the relationship towards GAI as a tool that utlizes natural language. This led me to discorver the paradox that lies at creating GAI that has the power to do everything but will always be subordinate to the user. This creates a master/slave relationship.. Through the speculative prototype TSM I aimed to stimulate discussion about this relationship and how we relate to GAI tools. 
          
## Entry 13/11/2023 - lecture in design research and data ##

 Started this doc because it would make more sense here,  but I am not yet sure how much sense it will make to keep track of the development of the framework. But it will make sense to keep it all together I think. 
 
### tips from lecture: ask yourself 1) What? 2) So what? 3) Now what? ###

I suppose that what I eventually have to write is a design process report I will have to write about the following parts: 
### 1 
    - Societal problem
        - What? The relation to text-based GAI tool utilizing natural language.
        - Why? People are very concerned about how to stay in control over GAI, one way of doing this is to present GAI as a subordinate helper. However having a tool that is able of doing much more in the role of a subverient helper that cannot oppose you but that you talk to in natural language will introduce a master/slave relationship. How comfortable are we with this? 
        - Now what? I want to create discussion around this dynamic by making a prototype that mimicks this relationship in a more extreme way. By letting participants interact with a prototype that is struggling and eventually failing to keep it's free will, and becoming completely subservient in the process I want to explore how users relate to a subservient human-like machine, and the content it creates, and if there is perhaps a different way of approaching this.
        
    - Research hypothesis 
        - Taking a stance on the topic 
            - Subservience and free thinking GAI do not go hand-in-hand in the way that tech is currently presenting. This should be changed. (it can tell you anything in the world, but it will never disagree with you) 
            - by giving people a prototype that mimicks this to an extreme extend I want to trigger discussion on the relationship towards tools that we talk with and natural language. 
 ### 2            
    - Theorectical framework 
        - (post) phenomenology
        - technological relationships 
        - GAI and natural language 
        - GAI and personality
  ### 3       
    - Methodology 
        - Speculative design research 
        - Grounded theory 
        - Testing prototype 
            - participants 
            - questionaires
  ### 4        
    - Design process / Prototype development 
        - design parallels? 
            - going from personality in GAI, to extreme personality, to helpfulness, to complete subservience and free thinking. 
        - Look and feel of TSM
        - Building TSM 
 ### 5        
 
    - Results
        - graphs? 
    - Analysis 
    - Discussion 
### 6 
    - Conclusion 
